{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e840bdb0-2b8c-4045-b106-10a5313a4f4b",
   "metadata": {},
   "source": [
    "# Chapitre 3 - Modélisation statistique et économétrique des données départementales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e777dd-750b-4d87-a914-702bda7da744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramètre(s) du notebook\n",
    "\n",
    "# VERBOSE=True\n",
    "VERBOSE=False\n",
    "\n",
    "OPTIONS=\"\"\n",
    "if not VERBOSE:\n",
    "    OPTIONS=\"--quiet\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8b01e4-4a70-46ef-8904-4449c3b95a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import scipy.stats as stats\n",
    "from statsmodels.formula.api import ols\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7417eb0-1fbf-40b9-8124-147eeb71682f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r donnees_2018_hab\n",
    "%store -r donnees_2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c4801d-1b86-4f17-a42e-7ae430f92462",
   "metadata": {},
   "outputs": [],
   "source": [
    "donnees_2018_hab=donnees_2018_hab.drop(columns=['REG', 'Libellé','Crim_Del_PN_GN','Crim_Del_GN_hab','Crim_Del_PN_hab'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05fcdbe-ca2b-477e-bee2-9a3dad672bbe",
   "metadata": {},
   "source": [
    "# 1 - Approche économétrique\n",
    "\n",
    "Dans une première approché économétrique, nous essayons de prédire le nombre de crimes à partir uniquement du nombre de boucherie, puis en rajoutant des variables de contrôles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db661dd4-5de4-43ce-8969-64a8e2c0ce22",
   "metadata": {},
   "source": [
    "## 1-A Régression linéaire simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6151375-a33a-4a1a-a957-8d041806ea09",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"*\"*100)\n",
    "print(\"\"*100)\n",
    "print(\"Une régression simple sur l'ensemble des données non transformées :\")\n",
    "print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8319b6e-fdae-4e44-a902-f301ba611518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from https://towardsdatascience.com/simulating-replicating-r-regression-plot-in-python-using-sklearn-4ee48a15b67\n",
    "\n",
    "lm = LinearRegression()\n",
    "\n",
    "X=donnees_2018_hab[[ 'Nb_Boucherie_dep_hab']]\n",
    "Y=donnees_2018_hab[['Crim_Del_PN_GN_hab']]\n",
    "\n",
    "lm.fit(X, Y)\n",
    "\n",
    "\n",
    "# # for predictions\n",
    "predictions = lm.predict(X)\n",
    "\n",
    "beta_hat = [lm.intercept_.tolist()] + lm.coef_.tolist()\n",
    "print(\"coefficients : \",beta_hat)\n",
    "plt.scatter(X, Y)\n",
    "plt.xlabel(\"Nb_Boucherie_dep_hab\")\n",
    "plt.ylabel(\"Crim_Del_PN_GN_hab\")\n",
    "plt.title('Nb_Boucherie_dep_hab*Crim_Del_PN_GN_hab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507fd547-ac70-4be5-a2b6-721bfe530b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_with_statsmodels = ols(\" Crim_Del_PN_GN_hab ~  Nb_Boucherie_dep_hab\", data = donnees_2018_hab).fit()\n",
    "print(reg_with_statsmodels.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ebb8fa-325e-4bc8-b793-f8699454a005",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" \"*100)\n",
    "print(\" ----- INTERPRETATION : le modèle est invalide par la F-statistique et la non significativité du coefficient associé à Nb_Boucherie_dep_hab   (R² quasi nul) \")\n",
    "print(\" ----- INTERPRETATION : le R² est quasi nul, suggérant la non-corrélation des deux variables, et ainsi un faible pouvoir prédictif \")\n",
    "print(\" ----- INTERPRETATION : on tente de rajouter des régresseurs supplémentaires pour gagner en prédictivité.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b479f4-627f-404f-b1fe-c4447cca1fed",
   "metadata": {},
   "source": [
    "## 1-B Régression linéaire multiple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629f9c64-4886-4267-b159-0a46643f24c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"*\"*100)\n",
    "print(\" \"*100)\n",
    "print(\"Une régression multiple sur l'ensemble des données non transformées :\")\n",
    "print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e928df-2a5d-4e34-9b6b-09bb23efd334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from https://towardsdatascience.com/simulating-replicating-r-regression-plot-in-python-using-sklearn-4ee48a15b67\n",
    "lm = LinearRegression()\n",
    "\n",
    "X=donnees_2018_hab[['MED18', 'TP6018', 'D118', 'D918', 'RD18', 'T1_2018',\n",
    "       'Nb_PN_GN_dep_100k_hab', 'Nb_Boucherie_dep_hab']]\n",
    "Y=donnees_2018_hab[['Crim_Del_PN_GN_hab']]\n",
    "\n",
    "lm.fit(X, Y)\n",
    "\n",
    "\n",
    "\n",
    "# # for predictions\n",
    "predictions = lm.predict(X)\n",
    "print(\"variables explicatives\",['MED18', 'TP6018', 'D118', 'D918', 'RD18', 'T1_2018',\n",
    "       'Nb_PN_GN_dep_100k_hab', 'Nb_Boucherie_dep_hab'])\n",
    "print(\"coefficients\",lm.coef_)\n",
    "print(\"constante\",lm.intercept_)\n",
    "\n",
    "print(\"R² :\",sklearn.metrics.r2_score(Y,predictions ))\n",
    "print(\" \"*100)\n",
    "print(\" ----- INTERPRETATION : nous obtenons un meilleur pouvoir prédictif par le modèle, notamment liée à la forte corrélation entre le ratio interdécile et la variable à expliquer  \")\n",
    "print(\" ----- INTERPRETATION : nous regarderons ultérieurement les résidus  \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3cd5bc-a9b5-45ac-8923-629e0a08be48",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"*\"*100)\n",
    "print(\" \"*100)\n",
    "print(\"Une régression multiple sur l'ensemble des données non transformées part statsmodel :\")\n",
    "print(\" Crim_Del_PN_GN_hab ~  Nb_Boucherie_dep_hab + MED18 + TP6018 + D118 + D918 + RD18  + T1_2018+    Nb_PN_GN_dep_100k_hab\")\n",
    "print(\"-\"*100)\n",
    "\n",
    "formula_reg= \" Crim_Del_PN_GN_hab ~  Nb_Boucherie_dep_hab + MED18 + TP6018 + D118 + D918 + RD18  + T1_2018+    Nb_PN_GN_dep_100k_hab\"\n",
    "reg_with_statsmodels = ols( formula_reg, data = donnees_2018_hab).fit()\n",
    "print(reg_with_statsmodels.summary())\n",
    "\n",
    "\n",
    "print(\" \"*100)\n",
    "print(\" ----- INTERPRETATION : on obtient de nouveau le même R² et les mêmes coefficients  \")\n",
    "print(\" ----- INTERPRETATION : Toutes les coefficients sont significatifs sauf Nb_Boucherie_dep_hab et Nb_PN_GN_dep_100k_hab \")\n",
    "print(\" ----- INTERPRETATION : On peut déjà esquisser l'hypothèse qu'il sera difficile de montrer un usage du nombre de boucherie sur le nombre des crimes\")\n",
    "print(\" ----- INTERPRETATION : La note [2] nous suggère un problème de multicollinéarité (potentiellement résoluble par une régression PLS)  ou un problème avec les variables numériques\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68eb97ec-58bb-44ed-bde9-7f8039f3924d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"*\"*100)\n",
    "print(\" \"*100)\n",
    "print(\"Retour sur les résidus :\")\n",
    "print(\"-\"*100)\n",
    "\n",
    "sns.residplot(predictions.reshape(-1),'Crim_Del_PN_GN_hab', data=donnees_2018_hab,lowess=True,\n",
    "                                  line_kws={'color': 'red', 'lw': 1, 'alpha': 1})\n",
    "plt.xlabel(\"Fitted values\")\n",
    "plt.title('Residual plot')\n",
    "\n",
    "print(\" ----- INTERPRETATION : pour deux départements quasi-similaires en termes socio-économiques, on peut constater un important écart de prédiction de 4000 crimes et délits  \")\n",
    "print(\" ----- INTERPRETATION : l'amplitude d'un résidu est de +/-2000 crimes et délits, sachant que la médiane parmi les départements est aux alentours de 4000 \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c17af4b-5e88-406e-a0f4-d86fd7ad663a",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = donnees_2018_hab[\"Crim_Del_PN_GN_hab\"] - predictions.reshape(-1)\n",
    "residuals\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
    "plt.title(\"Normal Q-Q Plot\")\n",
    "\n",
    "print(\" ----- INTERPRETATION : La normalité des résidus peut être conjecturée si l'on ne considère pas les points atypiques\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4da845-c6be-4b5b-929a-b73c42a5ad31",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_norm_residuals_abs_sqrt=np.sqrt(np.abs(residuals))\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "sns.regplot(predictions.reshape(-1), model_norm_residuals_abs_sqrt,\n",
    "              scatter=True,\n",
    "              lowess=True,\n",
    "              line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\n",
    "plt.ylabel(\"Standarized residuals\")\n",
    "plt.xlabel(\"Fitted value\")\n",
    "\n",
    "\n",
    "print(\" ----- INTERPRETATION : L'hypothèse d'homoscédasticité ne peut être faite ici.\")\n",
    "print(\" ----- INTERPRETATION : En présence d'hétéroscédasticité, l'ouvrage du Woolridge nous suggère d'utiliser le log (ou log(1+y)) pour : \")\n",
    "print(\" ----- INTERPRETATION : * les variables de population \")\n",
    "print(\" ----- INTERPRETATION : * les variables monétaires \")\n",
    "print(\" ----- INTERPRETATION : * les pourcentages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bfead7-868c-4435-b011-1bb9d7a8fce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"*\"*100)\n",
    "print(\" \"*100)\n",
    "print(\"Une régression multiple en cross validation avec des données non transformées\")\n",
    "print(\"-\"*100)\n",
    "\n",
    "scores = cross_val_score(lm, X, Y, scoring='r2', cv=5)\n",
    "print(\"R² obtenu en cross validation\",scores)\n",
    "\n",
    "print(\" \"*100)\n",
    "print(\" ----- INTERPRETATION : En utilisant la validation croisée, nous obtenons plusieurs R² très différents, de quasi-nulle à 0.72\")\n",
    "print(\" ----- INTERPRETATION : La forte variabilité du R², compte tenu du faible nombre d'invidividus dans l'échantillon, montre la sensibilité aux outliers\")\n",
    "print(\" ----- INTERPRETATION : Ceci présage également la présence de différents groupes hétérogènes comme dans le clustering dans le chapitre 2\")\n",
    "# R² can be negative with small datasets and cv fold https://stackoverflow.com/questions/23036866/scikit-learn-is-returning-coefficient-of-determination-r2-values-less-than-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1741be6f-a664-4bed-90ba-42e5cbaa4b36",
   "metadata": {},
   "source": [
    "## 1-C Régression linéaire multiple avec transformation log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4490a9-0e15-48e1-81fc-cbae3c86f263",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_donnees_2018_hab=donnees_2018_hab.copy()\n",
    "new_donnees_2018_hab['Crim_Del_PN_GN_hab'] = np.log(new_donnees_2018_hab['Crim_Del_PN_GN_hab'])\n",
    "new_donnees_2018_hab['RD18'] = np.log(new_donnees_2018_hab['RD18'])\n",
    "new_donnees_2018_hab['D918'] = np.log(new_donnees_2018_hab['D918'])\n",
    "new_donnees_2018_hab['D118'] = np.log(new_donnees_2018_hab['D118'])\n",
    "new_donnees_2018_hab['TP6018'] = np.log(new_donnees_2018_hab['TP6018'])\n",
    "new_donnees_2018_hab['MED18'] = np.log(new_donnees_2018_hab['MED18'])\n",
    "new_donnees_2018_hab['Nb_Boucherie_dep_hab'] = np.log(new_donnees_2018_hab['Nb_Boucherie_dep_hab'])\n",
    "new_donnees_2018_hab['Nb_PN_GN_dep_100k_hab'] = np.log(new_donnees_2018_hab['Nb_PN_GN_dep_100k_hab'])\n",
    "new_donnees_2018_hab['T1_2018'] = np.log(new_donnees_2018_hab['T1_2018'])\n",
    "\n",
    "formula_reg= \" Crim_Del_PN_GN_hab ~  Nb_Boucherie_dep_hab + MED18 + TP6018   + T1_2018 +    Nb_PN_GN_dep_100k_hab\"\n",
    "\n",
    "reg_with_statsmodels = ols( formula_reg, data = new_donnees_2018_hab).fit()\n",
    "print(reg_with_statsmodels.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13751e33-f873-42b3-b728-66b5a219f067",
   "metadata": {},
   "source": [
    "## 1-D Régression linéaire multiple avec transformation log et sans les outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651af4f3-bb40-434f-b5de-461fc6434dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_donnees_2018_hab=donnees_2018_hab[donnees_2018_hab['Crim_Del_PN_GN_hab']<6700]\n",
    "new_donnees_2018_hab['Crim_Del_PN_GN_hab'] = np.log(new_donnees_2018_hab['Crim_Del_PN_GN_hab'])\n",
    "new_donnees_2018_hab['RD18'] = np.log(new_donnees_2018_hab['RD18'])\n",
    "new_donnees_2018_hab['D918'] = np.log(new_donnees_2018_hab['D918'])\n",
    "new_donnees_2018_hab['D118'] = np.log(new_donnees_2018_hab['D118'])\n",
    "new_donnees_2018_hab['TP6018'] = np.log(new_donnees_2018_hab['TP6018'])\n",
    "new_donnees_2018_hab['MED18'] = np.log(new_donnees_2018_hab['MED18'])\n",
    "new_donnees_2018_hab['Nb_Boucherie_dep_hab'] = np.log(new_donnees_2018_hab['Nb_Boucherie_dep_hab'])\n",
    "new_donnees_2018_hab['Nb_PN_GN_dep_100k_hab'] = np.log(new_donnees_2018_hab['Nb_PN_GN_dep_100k_hab'])\n",
    "new_donnees_2018_hab['T1_2018'] = np.log(new_donnees_2018_hab['T1_2018'])\n",
    "\n",
    "# formula_reg= \" Crim_Del_PN_GN_hab ~  Nb_Boucherie_dep_hab + MED18 + TP6018   + T1_2018 +    Nb_PN_GN_dep_100k_hab\"\n",
    "formula_reg= \" Crim_Del_PN_GN_hab ~  Nb_Boucherie_dep_hab + Nb_PN_GN_dep_100k_hab  +T1_2018 + TP6018    \"\n",
    "reg_with_statsmodels = ols( formula_reg, data = new_donnees_2018_hab).fit()\n",
    "print(reg_with_statsmodels.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ee5998-4229-43c8-a373-d0b619d79ee6",
   "metadata": {},
   "source": [
    "# 2 - Approche Machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed411332-ba36-4f45-85a5-e31cd78664de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html\n",
    "# Both StandardScaler and MinMaxScaler are very sensitive to the presence of outliers.\n",
    "# MaxAbsScaler therefore also suffers from the presence of large outliers.\n",
    "\n",
    "\n",
    "\n",
    "# df_train, df_test = train_test_split(donnees_2018_hab, \n",
    "#                                      train_size = 0.7, \n",
    "#                                      test_size = 0.3\n",
    "#                                     )\n",
    "\n",
    "\n",
    "\n",
    "# pipeline = make_pipeline(RobustScaler(),  SVR(kernel='linear',C=1.0, epsilon=0.2))\n",
    "# pipeline.fit(X_train, y_train)\n",
    "# # vA = GridSearchCV(pipeline, param_grid=param_grid,\n",
    "# #                      scoring='roc_auc', cv=10, refit=True)\n",
    "# # vA.fit(X_train, y_train)\n",
    "\n",
    "# print(pipeline.predict(X_test))\n",
    "# print(y_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# new_X_train = X_train['Nb_Boucherie_dep_hab']\n",
    "# new_Y_train = y_train\n",
    "# new_X_test= X_test['Nb_Boucherie_dep_hab']\n",
    "# new_Y_test = y_test\n",
    "\n",
    "# sc_X = RobustScaler()\n",
    "# sc_Y = RobustScaler()\n",
    "# X_train_scaled = sc_X.fit_transform(new_X_train.values.reshape(-1, 1))\n",
    "# Y_train_scaled = sc_Y.fit_transform(new_Y_train.values.reshape(-1, 1))\n",
    "# X_test_scaled=sc_X.fit_transform(new_X_test.values.reshape(-1, 1))\n",
    "\n",
    "\n",
    "# svr = SVR(kernel ='linear')\n",
    "# svr.fit(X_train_scaled, Y_train_scaled)\n",
    "# plt.scatter(X_train_scaled, Y_train_scaled, color = 'blue')\n",
    "# plt.scatter(X_train_scaled, svr.predict(X_train_scaled), color = 'red')\n",
    "# plt.title('Crim_Del_PN_GN_hab vs  Nb_Boucherie_dep_hab  (SVR)')\n",
    "# plt.xlabel('Nb_Boucherie_dep_hab')\n",
    "# plt.ylabel('Crim_Del_PN_GN_hab')\n",
    "# plt.show()\n",
    "\n",
    "# print(sc_Y.inverse_transform(svr.predict(X_test_scaled).reshape(-1,1)))\n",
    "# print(new_Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d4e57b-a050-45bf-b4e6-f6986bfd0bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(donnees_2018_hab, \n",
    "                                     train_size = 0.7, \n",
    "                                     test_size = 0.3\n",
    "                                    )\n",
    "\n",
    "X=df_train['Nb_Boucherie_dep_hab'].values.reshape(-1,1)\n",
    "y=df_train['Crim_Del_PN_GN_hab'].values.reshape(-1,1)\n",
    "\n",
    "X_test=df_test['Nb_Boucherie_dep_hab'].values.reshape(-1,1)\n",
    "y_test=df_test['Crim_Del_PN_GN_hab'].values.reshape(-1,1)\n",
    "reg = make_pipeline(RobustScaler(),SGDRegressor(max_iter=1000, tol=1e-3))\n",
    "reg.fit(X, y)\n",
    "\n",
    "Y_pred=reg.predict(X_test)\n",
    "print(Y_pred)\n",
    "print(y_test)\n",
    "r2 = sklearn.metrics.r2_score(y_test,Y_pred )\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72af232-1aff-46ab-a77b-3bf0fb36c31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df_train[['MED18', 'TP6018', 'D118', 'D918', 'RD18', 'T1_2018',\n",
    "       'Nb_PN_GN_dep_100k_hab', 'Nb_Boucherie_dep_hab']].values.reshape(-1,8)\n",
    "y=df_train['Crim_Del_PN_GN_hab'].values.reshape(-1,1)\n",
    "\n",
    "X_test=df_test[['MED18', 'TP6018', 'D118', 'D918', 'RD18', 'T1_2018',\n",
    "       'Nb_PN_GN_dep_100k_hab', 'Nb_Boucherie_dep_hab']].values.reshape(-1,8)\n",
    "y_test=df_test['Crim_Del_PN_GN_hab'].values.reshape(-1,1)\n",
    "reg = make_pipeline(RobustScaler(),SGDRegressor(max_iter=1000, tol=1e-3))\n",
    "reg.fit(X, y)\n",
    "\n",
    "Y_pred=reg.predict(X_test)\n",
    "print(Y_pred)\n",
    "print(y_test)\n",
    "r2 = sklearn.metrics.r2_score(y_test,Y_pred )\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be7ab49-4d1b-4510-851d-1466b2e8ae2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
